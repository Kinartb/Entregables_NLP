Arturo Hinotroza Olivera

Ajustar finamente un LLM con PPO vs DPO vs ORPO utilizando el paquete PEFT
El ajuste fino de modelos de lenguaje a gran escala (LLMs) es una tarea esencial para adaptar modelos preentrenados a aplicaciones específicas. En este proyecto, se propone comparar tres técnicas de optimización por políticas: Proximal Policy Optimization (PPO), Deterministic Policy Optimization (DPO) y Optimistic Regularized Policy Optimization (ORPO), utilizando el paquete PEFT (Parameter-Efficient Fine-Tuning). El objetivo es evaluar cuál de estos métodos ofrece el mejor rendimiento en términos de eficiencia y precisión, especialmente en entornos con recursos computacionales limitados.
